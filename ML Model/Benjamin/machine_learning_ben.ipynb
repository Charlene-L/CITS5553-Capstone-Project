{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white' # Since I use a dark IDE\n",
    "\n",
    "# To allow multiple outputs per cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection  import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/benjamintan/Library/CloudStorage/OneDrive-TheUniversityofWesternAustralia/Master of Data Science/Year 2/Semester 2/CITS5553/CITS5553-Capstone-Project/ML Model/Benjamin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full data\n",
    "wba_data = pd.read_csv(\"./Data/wba_data_CLEAN.csv\")\n",
    "\n",
    "## Normal\n",
    "X_train_norm = pd.read_csv('./Data/Normal/X_train.csv')\n",
    "y_train_norm = pd.read_csv('./Data/Normal/y_train.csv')\n",
    "X_test_norm = pd.read_csv('./Data/Normal/X_test.csv')\n",
    "y_test_norm = pd.read_csv('./Data/Normal/y_test.csv')\n",
    "\n",
    "## SMOTE\n",
    "X_train_smote = pd.read_csv('./Data/Smote Large/X_train_smote.csv')\n",
    "y_train_smote = pd.read_csv('./Data/Smote Large/y_train_smote.csv')\n",
    "X_test_smote = pd.read_csv('./Data/Smote Large/X_test_smote.csv')\n",
    "y_test_smote = pd.read_csv('./Data/Smote Large/y_test_smote.csv')\n",
    "\n",
    "## Oversampling\n",
    "X_train_over = pd.read_csv('./Data/Oversampling Large/X_train_over.csv')\n",
    "y_train_over = pd.read_csv('./Data/Oversampling Large/y_train_over.csv')\n",
    "X_test_over = pd.read_csv('./Data/Oversampling Large/X_test_over.csv')\n",
    "y_test_over = pd.read_csv('./Data/Oversampling Large/y_test_over.csv')\n",
    "\n",
    "## ADASYN\n",
    "X_train_adasyn = pd.read_csv('./Data/Adasyn Large/X_train_adasyn.csv')\n",
    "y_train_adasyn = pd.read_csv('./Data/Adasyn Large/y_train_adasyn.csv')\n",
    "X_test_adasyn = pd.read_csv('./Data/Adasyn Large/X_test_adasyn.csv')\n",
    "y_test_adasyn = pd.read_csv('./Data/Adasyn Large/y_test_adasyn.csv')\n",
    "\n",
    "dfs = [X_train_norm,y_train_norm,X_test_norm,y_test_norm,\n",
    "X_train_smote, y_train_smote,X_test_smote,y_test_smote,\n",
    "X_train_over,y_train_over,X_test_over,y_test_over,\n",
    "X_train_adasyn,y_train_adasyn,X_test_adasyn,y_test_adasyn]\n",
    "\n",
    "for df in dfs:\n",
    "    df = df[[c for c in df.columns if c != \"Unnamed: 0\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test, vars = 'reduced', seed=42, cv_folds=5):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    if vars == 'full':\n",
    "        pass\n",
    "    elif vars == 'reduced':\n",
    "        feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "    elif vars == 'freqs':\n",
    "        freq_cols = [c for c in X_train.columns if c[:2] == \"f(\"]\n",
    "        X_train = X_train[freq_cols]\n",
    "        X_test = X_test[freq_cols]\n",
    "    elif vars == 'conts':\n",
    "        cont_cols = [c for c in X_train.columns if c[:2] != \"f(\"]\n",
    "        X_train = X_train[cont_cols]\n",
    "        X_test = X_test[cont_cols]\n",
    "    elif vars == 'freqs_reduced':\n",
    "        feat_select = ['f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "\n",
    "\n",
    "    # Creating hyperparameters ditionary\n",
    "    saga_grid = {'solver': ['saga'],\n",
    "                    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "                    'l1_ratio': [r/10 for r in list(range(0,10,1))]\n",
    "                    }\n",
    "    liblinear_grid = {'solver': ['liblinear'],\n",
    "                        'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    parameters = [saga_grid, liblinear_grid]\n",
    "\n",
    "    # Fit GridSearch\n",
    "    grid_log_reg = GridSearchCV(\n",
    "        LogisticRegression(random_state=seed),\n",
    "        parameters, \n",
    "        cv = 2\n",
    "    )\n",
    "    grid_log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best estimator\n",
    "    print(\"Best model: {}\".format(grid_log_reg.best_estimator_))\n",
    "    log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "    # Cross validation\n",
    "    cv_scores = cross_val_score(log_reg, X_train, y_train, cv=cv_folds)\n",
    "    # print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    print(\"{0}-fold cross validation:\\n  accuracy: {1}\\n  std dev: {2}\".format(cv_folds, round(cv_scores.mean(), 2), round(cv_scores.std(), 2)))\n",
    "\n",
    "    # Test set\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train, X_test, y_test, vars = 'reduced', seed=42, cv_folds=5):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    if vars == 'full':\n",
    "        pass\n",
    "    elif vars == 'reduced':\n",
    "        feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "    elif vars == 'freqs':\n",
    "        freq_cols = [c for c in X_train.columns if c[:2] == \"f(\"]\n",
    "        X_train = X_train[freq_cols]\n",
    "        X_test = X_test[freq_cols]\n",
    "    elif vars == 'conts':\n",
    "        cont_cols = [c for c in X_train.columns if c[:2] != \"f(\"]\n",
    "        X_train = X_train[cont_cols]\n",
    "        X_test = X_test[cont_cols]\n",
    "    elif vars == 'freqs_reduced':\n",
    "        feat_select = ['f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "\n",
    "    # Creating hyperparameters ditionary\n",
    "    params_linear = {'C': [0.1, 1, 10, 100, 1000], \n",
    "                'kernel': ['linear']} \n",
    "    params_nonlinear = {'C': [0.1, 1, 10, 100, 1000],\n",
    "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001,'auto'], \n",
    "                'kernel': ['poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "    parameters = [params_linear, params_nonlinear]\n",
    "\n",
    "\n",
    "    # Fit GridSearch\n",
    "    grid_svm = GridSearchCV(\n",
    "        SVC(random_state=seed),\n",
    "        parameters, \n",
    "        cv = 2\n",
    "    )\n",
    "    grid_svm.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best estimator\n",
    "    print(\"Best model: {}\".format(grid_svm.best_estimator_))\n",
    "    svm = grid_svm.best_estimator_\n",
    "\n",
    "    # Cross validation\n",
    "    cv_scores = cross_val_score(svm, X_train, y_train, cv=cv_folds)\n",
    "    # print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    print(\"{0}-fold cross validation:\\n  accuracy: {1}\\n  std dev: {2}\".format(cv_folds, round(cv_scores.mean(), 2), round(cv_scores.std(), 2)))\n",
    "\n",
    "    # Test set\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(classification_report(y_pred, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LogisticRegression(l1_ratio=0.0, penalty='none', random_state=42, solver='saga')\n",
      "5-fold cross validation:\n",
      "  accuracy: 0.95\n",
      "  std dev: 0.03\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        41\n",
      "           1       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.98        48\n",
      "   macro avg       0.94      0.99      0.96        48\n",
      "weighted avg       0.98      0.98      0.98        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LogisticRegression(l1_ratio=0.0, penalty='l1', random_state=42, solver='saga')\n",
      "5-fold cross validation:\n",
      "  accuracy: 0.93\n",
      "  std dev: 0.03\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        43\n",
      "           1       0.62      1.00      0.77         5\n",
      "\n",
      "    accuracy                           0.94        48\n",
      "   macro avg       0.81      0.97      0.87        48\n",
      "weighted avg       0.96      0.94      0.94        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 84 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n84 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1090, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1111, in _check_y\n    y = column_or_1d(y, warn=True)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1156, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (4250, 2) instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/23/sqlb91r144d65txwr_x9qnp80000gn/T/ipykernel_18851/2107265577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reduced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/23/sqlb91r144d65txwr_x9qnp80000gn/T/ipykernel_18851/3107222639.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(X_train, y_train, X_test, y_test, vars, seed, cv_folds)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgrid_log_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Extract best estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    850\u001b[0m                     )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 84 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n84 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1090, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1111, in _check_y\n    y = column_or_1d(y, warn=True)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1156, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (4250, 2) instead.\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_over, y_train_over, X_test_over, y_test_over, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 84 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n84 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1090, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1111, in _check_y\n    y = column_or_1d(y, warn=True)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1156, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (4250, 2) instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/23/sqlb91r144d65txwr_x9qnp80000gn/T/ipykernel_18851/899642899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'freqs_reduced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/23/sqlb91r144d65txwr_x9qnp80000gn/T/ipykernel_18851/3107222639.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(X_train, y_train, X_test, y_test, vars, seed, cv_folds)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgrid_log_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Extract best estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    850\u001b[0m                     )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 84 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n84 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1090, in check_X_y\n    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1111, in _check_y\n    y = column_or_1d(y, warn=True)\n  File \"/Applications/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1156, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (4250, 2) instead.\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_over, y_train_over, X_test_over, y_test_over, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LogisticRegression(random_state=42)\n",
      "5-fold cross validation:\n",
      "  accuracy: 0.99\n",
      "  std dev: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       747\n",
      "           1       1.00      0.99      1.00       753\n",
      "\n",
      "    accuracy                           1.00      1500\n",
      "   macro avg       1.00      1.00      1.00      1500\n",
      "weighted avg       1.00      1.00      1.00      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LogisticRegression(random_state=42)\n",
      "5-fold cross validation:\n",
      "  accuracy: 0.89\n",
      "  std dev: 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.88       810\n",
      "           1       0.83      0.90      0.87       690\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.87      0.87      0.87      1500\n",
      "weighted avg       0.88      0.87      0.87      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SVC(C=10, kernel='linear', random_state=42)\n",
      "5-fold cross validation:\n",
      "  accuracy: 1.0\n",
      "  std dev: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       750\n",
      "           1       1.00      1.00      1.00       750\n",
      "\n",
      "    accuracy                           1.00      1500\n",
      "   macro avg       1.00      1.00      1.00      1500\n",
      "weighted avg       1.00      1.00      1.00      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support_vector_machine(X_train_over, y_train_over, X_test_over, y_test_over, vars='reduced')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
