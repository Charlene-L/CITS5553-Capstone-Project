{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white' # Since I use a dark IDE\n",
    "\n",
    "# To allow multiple outputs per cell \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection  import train_test_split, GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/benjamintan/Library/CloudStorage/OneDrive-TheUniversityofWesternAustralia/Master of Data Science/Year 2/Semester 2/CITS5553/CITS5553-Capstone-Project/ML Model/Benjamin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full data\n",
    "wba_data = pd.read_csv(\"./Data/wba_data_CLEAN.csv\")\n",
    "\n",
    "## Normal\n",
    "X_train_norm = pd.read_csv('./Data/Normal/X_train.csv')\n",
    "y_train_norm = pd.read_csv('./Data/Normal/y_train.csv')\n",
    "X_test_norm = pd.read_csv('./Data/Normal/X_test.csv')\n",
    "y_test_norm = pd.read_csv('./Data/Normal/y_test.csv')\n",
    "\n",
    "## SMOTE\n",
    "X_train_smote = pd.read_csv('./Data/Smote Large/X_train_smote.csv')\n",
    "y_train_smote = pd.read_csv('./Data/Smote Large/y_train_smote.csv')\n",
    "X_test_smote = pd.read_csv('./Data/Smote Large/X_test_smote.csv')\n",
    "y_test_smote = pd.read_csv('./Data/Smote Large/y_test_smote.csv')\n",
    "\n",
    "## Oversampling\n",
    "X_train_over = pd.read_csv('./Data/Oversampling Large/X_train_over.csv')\n",
    "y_train_over = pd.read_csv('./Data/Oversampling Large/y_train_over.csv')\n",
    "X_test_over = pd.read_csv('./Data/Oversampling Large/X_test_over.csv')\n",
    "y_test_over = pd.read_csv('./Data/Oversampling Large/y_test_over.csv')\n",
    "\n",
    "## ADASYN\n",
    "X_train_adasyn = pd.read_csv('./Data/Adasyn Large/X_train_adasyn.csv')\n",
    "y_train_adasyn = pd.read_csv('./Data/Adasyn Large/y_train_adasyn.csv')\n",
    "X_test_adasyn = pd.read_csv('./Data/Adasyn Large/X_test_adasyn.csv')\n",
    "y_test_adasyn = pd.read_csv('./Data/Adasyn Large/y_test_adasyn.csv')\n",
    "\n",
    "## Generated\n",
    "X_train_gen = pd.read_csv('./Data/VAE (Experimental)/X_gen_train.csv', usecols = X_train_norm.columns)\n",
    "y_train_gen = pd.read_csv('./Data/VAE (Experimental)/X_gen_train.csv', usecols = ['OverallPoF'])\n",
    "X_test_gen = pd.read_csv('./Data/VAE (Experimental)/X_gen_test.csv', usecols = X_train_norm.columns)\n",
    "y_test_gen = pd.read_csv('./Data/VAE (Experimental)/X_gen_test.csv', usecols=['OverallPoF'])\n",
    "\n",
    "\n",
    "\n",
    "dfs = [X_train_norm,y_train_norm,X_test_norm,y_test_norm,\n",
    "X_train_smote, y_train_smote,X_test_smote,y_test_smote,\n",
    "X_train_over,y_train_over,X_test_over,y_test_over,\n",
    "X_train_adasyn,y_train_adasyn,X_test_adasyn,y_test_adasyn]\n",
    "\n",
    "# Delete Unnamed: 0 columns if they are there\n",
    "for df in dfs:\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train, X_test, y_test, vars = 'reduced', seed=42, cv_folds=5):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    if vars == 'full':\n",
    "        pass\n",
    "    elif vars == 'reduced':\n",
    "        feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "    elif vars == 'freqs':\n",
    "        freq_cols = [c for c in X_train.columns if c[:2] == \"f(\"]\n",
    "        X_train = X_train[freq_cols]\n",
    "        X_test = X_test[freq_cols]\n",
    "    elif vars == 'conts':\n",
    "        cont_cols = [c for c in X_train.columns if c[:2] != \"f(\"]\n",
    "        X_train = X_train[cont_cols]\n",
    "        X_test = X_test[cont_cols]\n",
    "    elif vars == 'freqs_reduced':\n",
    "        feat_select = ['f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "\n",
    "\n",
    "    # Creating hyperparameters ditionary\n",
    "    saga_grid = {'solver': ['saga'],\n",
    "                    'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "                    'l1_ratio': [r/10 for r in list(range(0,10,1))]\n",
    "                    }\n",
    "    liblinear_grid = {'solver': ['liblinear'],\n",
    "                        'penalty': ['l1', 'l2']}\n",
    "    \n",
    "    parameters = [saga_grid, liblinear_grid]\n",
    "\n",
    "    # Fit GridSearch\n",
    "    grid_log_reg = GridSearchCV(\n",
    "        LogisticRegression(random_state=seed),\n",
    "        parameters, \n",
    "        cv = 2\n",
    "    )\n",
    "    grid_log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best estimator\n",
    "    print(\"Best model: {}\".format(grid_log_reg.best_estimator_))\n",
    "    log_reg = grid_log_reg.best_estimator_\n",
    "\n",
    "    # Cross validation\n",
    "    cv_scores = cross_val_score(log_reg, X_train, y_train, cv=cv_folds)\n",
    "    print(\"{0}-fold cross validation:\\n  accuracy: {1}\\n  std dev: {2}\".format(cv_folds, round(cv_scores.mean(), 2), round(cv_scores.std(), 2)))\n",
    "\n",
    "    # Train set\n",
    "    print(\"Training Data\")\n",
    "    y_train_pred = log_reg.predict(X_train)\n",
    "    print(classification_report(y_train_pred, y_train))\n",
    "\n",
    "    # Test set\n",
    "    print(\"Test Data\")\n",
    "    y_test_pred = log_reg.predict(X_test)\n",
    "    print(classification_report(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train, X_test, y_test, vars = 'reduced', seed=42, cv_folds=5):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    if vars == 'full':\n",
    "        pass\n",
    "    elif vars == 'reduced':\n",
    "        feat_select = ['TPP', 'TympType', 'OAE1', 'OAE1.4', 'OAE2', 'OAE2.8', 'OAE4','f(408.4789)', 'f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "    elif vars == 'freqs':\n",
    "        freq_cols = [c for c in X_train.columns if c[:2] == \"f(\"]\n",
    "        X_train = X_train[freq_cols]\n",
    "        X_test = X_test[freq_cols]\n",
    "    elif vars == 'conts':\n",
    "        cont_cols = [c for c in X_train.columns if c[:2] != \"f(\"]\n",
    "        X_train = X_train[cont_cols]\n",
    "        X_test = X_test[cont_cols]\n",
    "    elif vars == 'freqs_reduced':\n",
    "        feat_select = ['f(2593.6791)', 'f(2378.4142)', 'f(2310.7054)', 'f(7127.1897)', 'f(865.5366)', 'f(6727.1713)', 'f(226.0000)', 'f(458.5020)', 'f(500.0000)', 'f(1029.3022)', 'f(5993.2283)', 'f(1887.7486)', 'f(1373.9536)', 'f(667.4199)', 'f(2747.9073)', 'f(1296.8396)', 'f(577.6763)', 'f(1155.3527)', 'f(1090.5077)']\n",
    "        X_train = X_train[feat_select]\n",
    "        X_test = X_test[feat_select]\n",
    "\n",
    "    # Creating hyperparameters ditionary\n",
    "    params_linear = {'C': [0.1, 1, 10, 100, 1000], \n",
    "                'kernel': ['linear']} \n",
    "    params_nonlinear = {'C': [0.1, 1, 10, 100, 1000],\n",
    "                'gamma': [1, 0.1, 0.01, 0.001, 0.0001,'auto'], \n",
    "                'kernel': ['poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "    parameters = [params_linear, params_nonlinear]\n",
    "\n",
    "\n",
    "    # Fit GridSearch\n",
    "    grid_svm = GridSearchCV(\n",
    "        SVC(random_state=seed),\n",
    "        parameters, \n",
    "        cv = 2\n",
    "    )\n",
    "    grid_svm.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best estimator\n",
    "    print(\"Best model: {}\".format(grid_svm.best_estimator_))\n",
    "    svm = grid_svm.best_estimator_\n",
    "\n",
    "    # Cross validation\n",
    "    cv_scores = cross_val_score(svm, X_train, y_train, cv=cv_folds)\n",
    "    # print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (cv_scores.mean(), cv_scores.std()))\n",
    "    print(\"{0}-fold cross validation:\\n  accuracy: {1}\\n  std dev: {2}\".format(cv_folds, round(cv_scores.mean(), 2), round(cv_scores.std(), 2)))\n",
    "\n",
    "    # Train set\n",
    "    print(\"Training Data\")\n",
    "    y_train_pred = svm.predict(X_train)\n",
    "    print(classification_report(y_train_pred, y_train))\n",
    "\n",
    "    # Test set\n",
    "    print(\"Test Data\")\n",
    "    y_test_pred = svm.predict(X_test)\n",
    "    print(classification_report(y_test_pred, y_test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_over, y_train_over, X_test_over, y_test_over, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_over, y_train_over, X_test_over, y_test_over, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_adasyn, y_train_adasyn, X_test_adasyn, y_test_adasyn, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(X_train_adasyn, y_train_adasyn, X_test_adasyn, y_test_adasyn, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='freqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_over, y_train_over, X_test_over, y_test_over, vars='freqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_norm, y_train_norm, X_test_norm, y_test_norm, vars='freqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_over, y_train_over, X_test_over, y_test_over, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='freqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_smote, y_train_smote, X_test_smote, y_test_smote, vars='freqs_reduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_adasyn, y_train_adasyn, X_test_adasyn, y_test_adasyn, vars='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_adasyn, y_train_adasyn, X_test_adasyn, y_test_adasyn, vars='freqs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine(X_train_adasyn, y_train_adasyn, X_test_adasyn, y_test_adasyn, vars='freqs_reduced')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
