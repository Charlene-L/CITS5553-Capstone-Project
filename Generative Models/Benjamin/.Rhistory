# Calculate MSE
mean((ridge.pred - y.test)^2)
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
# model <- glmnet(x[train], y[train], alpha = 0, lambda = grid)
plot(cv.out)
# Finding best lambda -> minimum
bestlam <- cv.out$lambda.min
bestlam
model <- glmnet(x, y, alpha = 1)
ridge.pred <- predict(model, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
bestlam <- cv.out$lambda.min
print(paste("best lambda:", bestlam))
plot(cv.out)
lasso.pred <- predict(model, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
library(splines)
dat <- ISLR::Wage
# bs(x, …) # For any degree splines
# ns(x, …) # For *natural* cubic splines
# not much difference... ns() a bit smoother - extends functions a bit past boundary to 'smoothen'
spline.model <- lm(wage ~
bs(age , knots = c(60, 92, 116, 140, 164)),
data = dat)
# degree=3 by default (cubic piecewise polynomial)
dat$pred.temp <- predict(spline.model, dat)
dat %>% ggplot() +
geom_point(aes(x = age, y = wage)) +
geom_line(aes(x=age, y=pred.temp), colour = "red") +
theme_minimal()
for(deg in seq(3,6)){
fit <- lm(wage~bs(age, degree = 3, df = deg), data=dat)
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)
plot(wage~age,
data=Wage,
col="gray",
main=paste("BS | deg: 3 | DF:", deg))
lines(age.grid, pred$fit, lwd=2)
}
for(deg in seq(3,6)){
fit <- lm(wage~bs(age, degree = 3, df = deg), data=dat)
age.lim <- range(dat$age)
age.grid <- seq(age.lim[1], age.lim[2])
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)
plot(wage~age,
data=Wage,
col="gray",
main=paste("BS | deg: 3 | DF:", deg))
lines(age.grid, pred$fit, lwd=2)
}
for(deg in seq(3,6)){
fit <- lm(wage~bs(age, degree = 3, df = deg), data=dat)
age.lim <- range(dat$age)
age.grid <- seq(age.lim[1], age.lim[2])
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)
plot(wage~age,
data=dat,
col="gray",
main=paste("BS | deg: 3 | DF:", deg))
lines(age.grid, pred$fit, lwd=2)
}
par(mfrow=c(2,2))
for(deg in seq(3,6)){
fit <- lm(wage~bs(age, degree = 3, df = deg), data=dat)
age.lim <- range(dat$age)
age.grid <- seq(age.lim[1], age.lim[2])
pred <- predict(fit, newdata=list(age=age.grid), se=TRUE)
plot(wage~age,
data=dat,
col="gray",
main=paste("BS | deg: 3 | DF:", deg))
lines(age.grid, pred$fit, lwd=2)
}
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
simul <-  sampling(test1, algorithm="Fixed_param")
print(simul, c("l1", "l2"), digits=4)
l1 <- unlist(extract(simul, "l1"))
l2 <- unlist(extract(simul, "l2"))
hist(l1, breaks="FD")
hist(l2, breaks="FD")
plot(l1, l2, pch=".")
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
knitr::opts_chunk$set(echo = TRUE,
tidy = TRUE)
cat(readLines("Ex1.bug"), sep="\n")
cat(readLines("Ex1.bug"), sep="\n")
knitr::opts_chunk$set(echo = TRUE,
tidy = TRUE)
cat(readLines("Ex1.bug"), sep="\n")
library(rjags)
install.packages('rjags')
library(rjags)
library(rjags)
library(rjags)
library(rjags, dependencies=TRUE)
install.packages(rjags, dependencies=TRUE)
install.packages("rjags", dependencies=TRUE)
library(rjags)
library(rjags)
library(rjags)
library(rjags)
library(rjags)
mod <- jags.model("Ex1.bug", n.chains=1)
out <- coda.samples(mod, c("Y", "P2"), n.iter=10000)
summary(out)
plot(out, trace=FALSE)
cat(readLines("Ex2-jags.bug"), sep="\n")
library(rjags)
mod <- jags.model("Ex2-jags.bug", n.chains = 1)
out <- coda.samples(mod, c("number", "Y[1]", "check"), n.iter=10000)
summary(out)
plot(out, trace=FALSE)
knitr::opts_chunk$set(echo = TRUE,
tidy = TRUE)
library(rstan)
library(bayesplot)
library(rstan)
library(bayesplot)
simul1 <- sampling(binomial, algorithm="Fixed_param")
print(simul1, c("y", "p"), digits=5)
simul2 <- sampling(gamma, algorithm="Fixed_param")
print(simul2, pars=c("number"), digits = 5)
1-pgamma(1000, 4, 0.04)
pgamma(1000, 4, 0.04, lower.tail = FALSE)
simul3 <- sampling(gamma2, algorithm="Fixed_param")
print(simul3, c("y", "z", "number"), digits=5)
?real
y
y
y
y
y
y
y
y
y
knitr::opts_chunk$set(echo = TRUE,
tidy = TRUE)
dat <- iris
View(dat)
dat <- iris
pairs(dat)
1 % z
1 % 5
1%5
1%2
5%1
10%3
10%mod% 3
10%%3
1%%4
wba <- read.csv("WBA_data.csv")
library(dplyr)
library(magrittr)
library(tidyr)
library(tidyverse)
library(ggplot2)
wba <- read.csv("WBA_data.csv")
wba <- read.csv("WBA_data.csv") %>%
filter(-Gender)
wba <- read.csv("WBA_data.csv") %>%
select(-Subject)
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
tidy = TRUE)
library(rstan)
library(bayesplot)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
data.in <- list(n=20, y=12, alpha=0.5, beta=0.5)
model.fit1 <- sampling(BinMod_beta, data=data.in)
View(data.in)
print(model.fit1, pars="p", probs=c(0.1,0.5,0.9))
print(model.fit1, pars="p", probs=c(0.1,0.5,0.9), digits=5)
check_hmc_diagnostics(model.fit1)
posterior <- as.array(model.fit1)
color_scheme_set("red")
mcmc_intervals(posterior, pars="p")
mcmc_areas(posterior, pars="p", point_est="mean")
color_scheme_set("mix-blue-red")
mcmc_trace(posterior, pars="p")
mcmc_acf(posterior, pars="p")
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(magrittr)
library(dplyr)
library(tidyr)
## Run Simulation
simul_day <- sampling(day_infect, algorithm="Fixed_param", iter=40000, seed=69)
## Run Simulation
simul_day <- sampling(day_infect, algorithm="Fixed_param", iter=40000, seed=69)
## Run Simulation
simul_day <- sampling(day_infect, algorithm="Fixed_param", iter=40000, seed=69)
## Run Simulation
simul_day <- sampling(day_infect, algorithm="Fixed_param", iter=40000, seed=69)
## Simulation output
print(simul_day, c("num_infect", "p_infect", "infectiousness_rate" , "num_people_met"), digits=4)
## Simulation output
print(simul_day, c("num_infect", "infect", "infect_rate" , "num_pplk"), digits=4)
## Simulation output
print(simul_day, c("num_infect", "infect", "infect_rate" , "num_ppl"), digits=4)
## Check to ensure distribution was uniform and outcomes were correct
u1 <- unlist(extract(simul_day, "num_people_met"))
?extract
?unlist
?extract
extract(simul_day)
rstan::extract(simul_day)
## Check to ensure distribution was uniform and outcomes were correct
u1 <- unlist(rstan::extract(simul_day, "num_people_met"))
## Check to ensure distribution was uniform and outcomes were correct
u1 <- unlist(rstan::extract(simul_day, "num_ppl"))
barplot(table(u1))
u1
library(ggplot2)
## Check to ensure distribution was uniform and outcomes were correct
u1 <- unlist(rstan::extract(simul_day, "num_ppl"))
table(u1) %>% ggplot(mapping=aes(x=num_ppl)) +
geom_bar()
library(rstan)
library(bayesplot)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
data.in <- list(n=197, y1=129, y2=149, alpha=0.5, beta=0.5)
model.fit1 <- sampling(BinMod_beta, data=data.in, iter = 50000)
print(model.fit1, pars=c("p1","p2", "odds", "probs"), probs=c(0.1,0.5,0.9), digits = 5)
?beta.select
??beta.select
install.packages(c("ProbBayes", "LearnBayes"))
library(ProbBayes)
var <- LearnBayes::beta.select(list(x=0.4, p = 0.025) , list(x= 0.6 , p=0.975))
cat("Alpha: " ,var[1] , "\nBeta: " , var[2])
library(ProbBayes)
var <- LearnBayes::beta.select(list(x=0.4, p = 0.025) , list(x= 0.6 , p=0.975))
print("Alpha: " ,var[1] , "\nBeta: " , var[2])
library(ProbBayes)
var <- LearnBayes::beta.select(list(x=0.4, p = 0.025) , list(x= 0.6 , p=0.975))
print("Alpha: " ,var[1] , "\nBeta: " , var[2])
library(ProbBayes)
var <- LearnBayes::beta.select(list(x=0.4, p = 0.025) , list(x= 0.6 , p=0.975))
print(paste("Alpha: " ,var[1] , "\nBeta: " , var[2]))
library(ProbBayes)
var <- LearnBayes::beta.select(list(x=0.4, p = 0.025) , list(x= 0.6 , p=0.975))
print(paste("Alpha: " ,var[1] , "Beta: " , var[2]))
# Sample size
n = 10
# Parameters of the beta distribution
alpha = var[1]
beta = var[2]
# Simulate some data
set.seed(1)
x = rbeta(n, alpha, beta)
# Plot Distribution
curve(dbeta(x,alpha,beta))
var
library(ProbBayes)
var <- LearnBayes::beta.select(
list(x=0.4, p = 0.025),
list(x= 0.6 , p=0.975)
)
print(paste("Alpha: ", var[1] , "Beta: " , var[2]))
library(ProbBayes)
var <- LearnBayes::beta.select(
list(x=0.4, p = 0.025),
list(x= 0.6 , p=0.975)
)
print(paste("Alpha: ", var[1] , "Beta: " , var[2]))
library(ProbBayes)
var <- LearnBayes::beta.select(
list(x=0.4, p = 0.025),
list(x= 0.6 , p=0.975)
)
print(paste("Alpha: ", var[1] , "Beta: " , var[2]))
library(ProbBayes)
var <- LearnBayes::beta.select(
list(x=0.4, p = 0.025),
list(x= 0.6 , p=0.975)
)
print(paste("Alpha: ", var[1] , "Beta: " , var[2]))
# Sample size
n = 10
# Parameters of the beta distribution
alpha = var[1]
beta = var[2]
# Simulate some data
set.seed(1)
x = rbeta(n, alpha, beta)
# Plot Distribution
curve(dbeta(x,alpha,beta))
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(bayesplot)
library(magrittr)
library(dplyr)
library(tidyr)
library(ggplot2)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
library(ProbBayes)
# Selection of Beta Prior Given Knowledge of Two Quantiles
quantile1 <- list(x=0.4, p=0.025)
quantile2 <- list(x=0.6, p=0.975)
var <- LearnBayes::beta.select(
quantile1,
quantile2
)
print(paste("Alpha: ", var[1] , "Beta: " , var[2]))
# Sample size
n = 10
# Parameters of the beta distribution
alpha = var[1]
beta = var[2]
# Simulate some data
set.seed(1)
x = rbeta(n, alpha, beta)
# Plot Distribution
curve(dbeta(x,alpha,beta))
abline(v=0.5, col="red", lty=2)
abline(v=c(0.4, 0.6), col="blue", lty=2)
dbeta(x, 0.4, 0.6)
# Sample size
n = 10
# Parameters of the beta distribution
alpha = var[1]
beta = var[2]
# Simulate some data
set.seed(1)
x = rbeta(n, alpha, beta)
# Plot Distribution
curve(dbeta(x,alpha,beta))
abline(v=0.5, col="red", lty=2)
abline(v=c(0.4, 0.6), col="blue", lty=2)
dbeta(x, 0.4, 0.6)
x
?rbeta
dbeta(x,alpha,beta)
knitr::opts_chunk$set(echo = TRUE)
library(rstan)
library(bayesplot)
correct_pos<-91
confirm_pos<-101
correct_neg<-303
confirm_neg<-305
data.in <- list(correct_pos = correct_pos, confirm_pos = confirm_pos, correct_neg = correct_neg, confirm_neg = confirm_neg)
fit2.1 <- sampling(task2.1, data = data.in, iter=10000)
print(fit2.1, pars = c("prob_pos"), digits = 4)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
print(fitted_model_t2, pars = c("p","test1_result"), probs = NULL, digits = 5)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
print(fitted_model_t2, pars = c("p","test1_result"), probs = NULL, digits = 5)
print(fitted_model_t2, pars = c("p","test1_result", "TNR", "TPR"), probs = NULL, digits = 5)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
print(fitted_model_t2, pars = c("p","test1_result", "TNR", "TPR"), probs = NULL, digits = 5)
print(fitted_model_t2, pars = c("p","test1_result"), probs = NULL, digits = 5)
data_in <- list(
n_pos = 101,
n_pos_detected = 91,
n_neg = 305,
n_neg_detected = 303,
alpha = 1.41,
beta = 547.12
)
fitted_model_t2 <- sampling(task_2, data = data_in, iter=10000)
print(fitted_model_t2, pars = c("p","test1_result"), probs = NULL, digits = 5)
correct_pos<-91
confirm_pos<-101
correct_neg<-303
confirm_neg<-305
data.in <- list(correct_pos = correct_pos, confirm_pos = confirm_pos, correct_neg = correct_neg, confirm_neg = confirm_neg)
fit2.1 <- sampling(task2.1, data = data.in, iter=10000)
print(fit2.1, pars = c("prob_pos"), digits = 4)
library(ggplot2)
library(dplyr)
library(Sleuth3)
krunnit <- case2101
krunnit$logArea = with(krunnit, log(Area))
krunnit.glm1 <- glm(cbind(Extinct, AtRisk-Extinct)~logArea, family=binomial(link=logit), data=krunnit)
krunnit$Pres = residuals(krunnit.glm1, "pearson")
hist(krunnit$Pres)
summary(krunnit.glm1)
krunnit
head(krunnit)
library(Sleuth3)
donner <- case2001
donner <- within(donner,{
Surv <- as.numeric(Status) - 1
SexN <- 2 -as.numeric(Sex) })
head(donner)
summary(donner)
with(donner, plot(Age, Surv, ylab="Survival", main="Age vs. Survival",
col=SexN+1, pch=SexN))
legend("topright", col=c(1,2),pch=c(0,1),
legend=c("Male","Female"))
with(donner, plot(jitter(Age), Surv, ylab="Survival", main="Age vs. Survival (Jitter)",
col=SexN+1, pch=SexN))
legend("topright", col=c(1,2),pch=c(0,1),
legend=c("Male","Female"))
pi.grid<-seq(0.001,0.998,by=0.001)
plot(pi.grid,log(pi.grid/(1-pi.grid)),type="l"
,col="red",
xlab=expression(pi),ylab=expression("logit("*pi*")"),
main = expression("Plot of the logit transformation of "*pi))
abline(h=0,v=c(0,1))
eta<-seq(-4,4,length=1000)
plot(eta,exp(eta)/(1+exp(eta)),xlab=expression(eta),
ylab=expression(pi),type="l", col = "red")
abline(0.5, 0)
abline(v=0)
donner$Sex <- relevel(donner$Sex, ref="Male")
donner.glm1<-glm(Status~Age+Sex,data=donner,family=binomial(link=logit))
with(donner, plot(Surv~jitter(Age),xlab="Age",ylab=expression(pi),col=SexN+1,pch=SexN))
age.grid<-with(donner, seq(min(Age),max(Age), by=0.1))
pi.fem<-exp(1.6331-0.0782*age.grid+1.5973)/(1+exp(1.6331-0.0782*age.grid+1.5973))
pi.male<-exp(1.6331-0.0782*age.grid)/(1+exp(1.6331-0.0782*age.grid))
lines(age.grid,pi.fem,type="l",col="red")
lines(age.grid,pi.male,type="l",col="black")
legend("topright",col=c(1,2),pch=c(0,1),legend=c("Male","Female"))
donner.glm2<-glm(Status~Age + Sex + Age:Sex, family=binomial(link=logit), data=donner)
summary(donner.glm2)
with(donner,plot(jitter(Age),Surv,col=SexN+1,pch=SexN, ylab=expression(pi)))
age.grid<-seq(min(donner$Age),max(donner$Age),by=0.1)
coefs<-coef(donner.glm2)
female.int<-coefs[1]+coefs[3]
female.slope<-coefs[2]+coefs[4]
fem.pi<-exp(female.int+female.slope*age.grid)/(1+exp(female.int+female.slope*age.grid))
male.pi<-exp(coefs[1]+coefs[2]*age.grid)/(1+exp(coefs[1]+coefs[2]*age.grid))
lines(age.grid,fem.pi,type="l",col="red")
lines(age.grid,male.pi,type="l",col="black")
legend("topright",col=c(1,2),pch=c(0,1),lty=1,legend=c("Male","Female"))
beta3.hat.obs<- -0.1616
beta3.null<-0
beta3.hat.se<-0.09426
2*pnorm(abs(beta3.hat.obs),beta3.null,beta3.hat.se, lower.tail=FALSE)
beta3.grid<-seq(-0.3,0.3,by=0.01)
plot(beta3.grid,dnorm(beta3.grid,beta3.null,beta3.hat.se),
type="l",xlab=expression(beta[3]),ylab=expression("Density of " * beta[3]))
abline(v=c(beta3.hat.obs,abs(beta3.hat.obs)),h=0)
donner.glm1$deviance
# 51.25628
donner.glm2$deviance
# 47.34637
library(dplyr)
library(tidyr)
library(magrittr)
library(ggplot2)
gen_df <- read.csv('gen_df.csv')
gen_df <- gen_df[-X]
gen_df <- gen_df[-'X']
gen_df <- gen_df[-X,]
gen_df <- read.csv('gen_df.csv',colClasses=c("NULL", NA, NA)
gen_df <- read.csv('gen_df.csv',colClasses=c("NULL", NA, NA))
gen_df <- read.csv('gen_df.csv',colClasses=c("NULL", NA, NA)
gen_df <- read.csv('gen_df.csv',colClasses=c("NULL", NA, NA))
gen_df <- read.csv('gen_df.csv')[1:]
gen_df <- read.csv('gen_df.csv')
wba_df <- read.csv('wba_data_CLEAN.csv')
cont_cols <- c('AgeY','TPP','OAE1','OAE1.4','OAE2','OAE2.8','OAE4','Pressure','SC','ECV','Gender','TympType','Ear coded','OverallPoF')
cont_cols <- c('AgeY','TPP','OAE1','OAE1.4','OAE2','OAE2.8','OAE4','Pressure','SC','ECV','Gender','TympType','Ear coded','OverallPoF')
melt(gen_df)
reshape::melt(gen_df)
reshape::melt(gen_df, id = cont_cols)
cont_cols <- c('AgeY','TPP','OAE1','OAE1.4','OAE2','OAE2.8','OAE4','Pressure','SC','ECV','Gender','TympType','Ear..coded','OverallPoF')
reshape::melt(gen_df, id = cont_cols)
cont_cols <- c('AgeY','TPP','OAE1','OAE1.4','OAE2','OAE2.8','OAE4','Pressure','SC','ECV','Gender','TympType','Ear.coded','OverallPoF')
